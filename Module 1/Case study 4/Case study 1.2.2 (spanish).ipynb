{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Caso-de-estudio-1.2.2:-Agrupamiento-espectral:-Agrupación-de-noticias\" data-toc-modified-id=\"Caso-de-estudio-1.2.2:-Agrupamiento-espectral:-Agrupación-de-noticias-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Caso de estudio 1.2.2: Agrupamiento espectral: Agrupación de noticias</a></span></li><li><span><a href=\"#Generación-de-la-base-de-datos-(Web-Scraping)\" data-toc-modified-id=\"Generación-de-la-base-de-datos-(Web-Scraping)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Generación de la base de datos (Web Scraping)</a></span></li><li><span><a href=\"#Importación-de-la-base-de-datos\" data-toc-modified-id=\"Importación-de-la-base-de-datos-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Importación de la base de datos</a></span></li><li><span><a href=\"#Generación-de-atributos\" data-toc-modified-id=\"Generación-de-atributos-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Generación de atributos</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caso de estudio 1.2.2: Agrupamiento espectral: Agrupación de noticias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "\n",
    "Este caso de estudio considera una base de datos de artículos de prensa, sobre diferentes temas, y usa _clusterización espectral_ para agruparlos dependiendo de la frecuencia de ciertas palabras. Este notebook proporciona el código para generar la base de datos, pero también puede enocontrar un ejemplo de base de datos en la carpeta `/Data`. Esta base de datos se generó el día 29 de mayo de 2020 mediante técnicas de minería de datos (_web scraping_).\n",
    "\n",
    "Este caso de estudio usa la librería [`mitie`](https://github.com/mit-nlp/MITIE), desarrollada en MIT. Todos los pasos para instalar tnato la librería como el modelo NER usado en este caso de estudio pueden encontrarse en la documentación online.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuración del notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T15:20:44.946232Z",
     "start_time": "2020-06-05T15:20:40.796717Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading NER model...\n",
      "\n",
      "Tags output by this NER model: ['PERSON', 'LOCATION', 'ORGANIZATION', 'MISC']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import csv\n",
    "\n",
    "#ML\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import cluster\n",
    "\n",
    "#Web scraping libraries\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#NLP libraries\n",
    "from mitie import *\n",
    "print(\"loading NER model...\")\n",
    "ner = named_entity_extractor('/Users/inigo/opt/anaconda3/lib/python3.7/site-packages/mitie/models/MITIE-models/english/ner_model.dat')\n",
    "print(\"\\nTags output by this NER model:\", ner.get_possible_ner_tags())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de la base de datos (Web Scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, artículos de 8 temas diferentes del periódico __The Guardian__ han sido recopilados. Los pasos a seguir para crear la base de datos son:\n",
    "\n",
    "1. Obtener el código fuente de la web principal de The Guardian, y almacenar los links a las secciones (temas) de interés.\n",
    "2. Iterar la lista de links y obtener la información de 10 artículos por sección (título y contenido).\n",
    "3. Guardar los artículos, títulos, y temas en archivos `.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:00:19.553806Z",
     "start_time": "2020-05-29T17:00:18.879784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: Elections_2020 (https://www.theguardian.com/us-news/us-elections-2020)\n",
      "Topic 2: World (https://www.theguardian.com/world)\n",
      "Topic 3: Environment (https://www.theguardian.com/us/environment)\n",
      "Topic 4: Soccer (https://www.theguardian.com/football)\n",
      "Topic 5: US_Politics (https://www.theguardian.com/us-news/us-politics)\n",
      "Topic 6: Business (https://www.theguardian.com/us/business)\n",
      "Topic 7: Tech (https://www.theguardian.com/us/technology)\n",
      "Topic 8: Science (https://www.theguardian.com/science)\n"
     ]
    }
   ],
   "source": [
    "UK_news_url = 'https://www.theguardian.com/uk'\n",
    "#Descargando los links de los diferentes temas\n",
    "html_data = requests.get(UK_news_url).text\n",
    "soup = BeautifulSoup(html_data, 'html.parser')\n",
    "url_topics = [el.find('a')['href'] for el in soup.find_all(class_ = 'subnav__item')[1:9]]\n",
    "topics = [el.text.strip('\\n').replace(' ','_') for el in soup.find_all(class_ = 'subnav-link')[1:9]]\n",
    "for i in range(len(topics)):\n",
    "    print('Topic {}: {} ({})'.format(i+1,topics[i],url_topics[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:01:08.527253Z",
     "start_time": "2020-05-29T17:00:19.556907Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elections_2020:\n",
      "Biden sets solemn tone as Trump waits 15 hours to mark Covid-19 milestone\n",
      "Revealed: conservative group fighting to restrict voting tied to powerful dark money network\n",
      "Twitter labels Trump's false claims with warning for first time\n",
      "'Feels good to be out of my house': Biden lays Memorial Day wreath in Delaware\n",
      "Why is Trump so restrained about the Biden sexual assault allegation?\n",
      "Swing states become partisan battlegrounds in America's fight against Covid-19\n",
      "Trump campaign focuses fire on Biden as pandemic undermines strategy\n",
      "'Obamagate': Fox News focuses on conspiracy theory rather than Covid-19\n",
      "‘The United States is broken as hell’ – the division in politics over race and class\n",
      "Socialism used to be a dirty word. Is America now ready to embrace it?\n",
      "Article count: 10\n",
      "\n",
      "World:\n",
      "Global report: South Korea postpones school reopening due to new outbreak\n",
      "'Gross incompentence at highest levels': ex-Obama adviser blasts Trump's Covid response\n",
      "Monkeys steal Covid-19 test samples from health worker in India\n",
      "China threatens 'countermeasures' against UK over Hong Kong crisis\n",
      "Italy considers charges over Malta's 'shocking' refusal to rescue migrants\n",
      "EU's greenhouse gas emissions continue to fall as coal ditched\n",
      "Police smash people-smuggling ring 'linked to Essex lorry deaths'\n",
      "Renault to cut 14,600 jobs as part of €2bn cost-saving plan\n",
      "Ethiopia's security forces accused of torture, evictions and killings – report\n",
      "Dominic Cummings cutout appears at rugby league match in Australia\n",
      "Article count: 20\n",
      "\n",
      "Environment:\n",
      "New Trump public land rules will let Alaska hunters kill bear cubs in dens\n",
      "US lets corporations delay paying environmental fines amid pandemic\n",
      "We are eating shrimp in record numbers. But for how much longer?\n",
      "Race, wealth and public spaces: US beaches are a new flashpoint of the lockdown\n",
      "Thousands of run-down US dams would kill people if they failed, study finds\n",
      "When the '500-year flood' hit Michigan, residents had to weigh risk of escape in a pandemic\n",
      "Firms ignoring climate crisis will go bankrupt, says Mark Carney\n",
      "Large heath butterflies return to Manchester after 150 years\n",
      "India's first 'green' village adapts to life without tourists\n",
      "'This makes Chinese medicine look bad': TCM supporters condemn illegal wildlife trade\n",
      "Article count: 30\n",
      "\n",
      "Soccer:\n",
      "Liverpool and Everton to oppose plans to play games at neutral venues\n",
      "Slavisa Jokanovic: 'Euro 92 was taken away from us. We were better than Denmark'\n",
      "What, when, where? Questions answered on Premier League's return\n",
      "Who wouldn’t want to spend a second consecutive summer day indoors?\n",
      "Golden Goal: Dietmar Hamann for Germany v England (2000)\n",
      "'People say you shouldn’t do this': Nathan Jones heads into fire at Luton\n",
      "'I dance to Joe Wicks every morning': football commentators in lockdown\n",
      "Germany rallied to save women's football – the FA should be embarrassed\n",
      "Premier League plans restart on 17 June with Manchester City v Arsenal\n",
      "Barcelona keen on Miralem Pjanic and refuse to give up on Lautaro Martínez\n",
      "Article count: 40\n",
      "\n",
      "US_Politics:\n",
      "The Minnesota paradox: how race divides prosperous Minneapolis\n",
      "'Trump knows nothing of Minneapolis': mayor hits back at president's tweets\n",
      "Trump signs executive order to narrow protections for social media platforms\n",
      "We shouldn't have to witness George Floyd's killing for it to spark outrage\n",
      "'Here's a bedsheet, make a parachute!' Republicans say, pushing us out of a plane\n",
      "Why Twitter should ban Donald Trump\n",
      "Trump's devoted new press secretary is no different from her predecessors\n",
      "'You have to respond forcefully': can Joe Biden fight Trump's brutal tactics?\n",
      "'We're expendable': black Americans pay the price as states lift lockdowns\n",
      "Only 9 articles found in \"US_Politics\"\n",
      "\n",
      "Business:\n",
      "US job losses pass 40m as coronavirus crisis sees claims rise 2.1m in a week\n",
      "Article count: 50\n",
      "Food-waste firm bags Oprah Winfrey and Katy Perry as investors\n",
      "Lufthansa agrees €9bn bailout with German government\n",
      "Tackling the dark side of the movie business: the blockchain startup ensuring film workers get paid\n",
      "Pizzas in the post: Shopify challenges Amazon for slice of lockdown trade\n",
      "Extra $600 in jobless pay offers many a lifeline – but will it be renewed?\n",
      "Exclusive: big pharma rejected EU plan to fast-track vaccines in 2017\n",
      "Jobless America: the coronavirus unemployment crisis in figures\n",
      "TikTok's first auteur: Zach King on his madcap micro movies\n",
      "The Zoom boom: how video-calling became a blessing – and a curse\n",
      "\n",
      "Tech:\n",
      "Twitter hides Donald Trump tweet for 'glorifying violence'\n",
      "Article count: 60\n",
      "Zuckerberg says Facebook won't be 'arbiters of truth' after Trump threat\n",
      "Donald Trump's move against Twitter factchecking could backfire\n",
      "New vulnerability allows users to 'jailbreak' iPhones\n",
      "The Chaser goes viral with provocative post mocking Zuckerberg’s position on Facebook factchecking\n",
      "Work From Home song peaks again as listening alters under lockdown\n",
      "Canada court ruling allows US extradition case of Huawei executive to proceed\n",
      "Huge rise in hacking attacks on home workers during lockdown\n",
      "EasyJet reveals cyber-attack exposed 9m customers' details\n",
      "Zoom hacker streams child sex abuse footage to Plymouth children\n",
      "\n",
      "Science:\n",
      "Virgin Orbit looks into cause of LauncherOne test failure\n",
      "Article count: 70\n",
      "Easing the lockdown: how will we know if infections bounce back?\n",
      "NHS to increase number of Covid-19 patients receiving antibody therapy\n",
      "Questions raised over hydroxychloroquine study which caused WHO to halt trials for Covid-19\n",
      "Allosaurus dinosaur suspected to be scavenging cannibal\n",
      "Hydroxychloroquine and coronavirus: a guide to the scientific studies so far\n",
      "UK minister hails 'game-changing' coronavirus immunity test\n",
      "Italian doctors find link between Covid-19 and inflammatory disorder\n",
      "Manchester's pubs pine for the end of lockdown\n",
      "Coronavirus in England: half of those with symptoms not isolating\n"
     ]
    }
   ],
   "source": [
    "def save_to_txt(filename, content):\n",
    "    '''\n",
    "    Creates a new .txt file with as specific name in the Data directory\n",
    "    '''\n",
    "    with open(r\"Data/{}.txt\".format(filename), \"w\") as f:\n",
    "        print(content, file=f)\n",
    "\n",
    "article_titles = []\n",
    "article_contents = []\n",
    "article_topics = []\n",
    "articles_per_topic = 10\n",
    "n = 1\n",
    "for topic, url_topic in list(zip(topics,url_topics)):\n",
    "    #Getting the first 15\n",
    "    soup = BeautifulSoup(requests.get(url_topic).text, 'html.parser')\n",
    "    url_articles = [el.find('a')['href'] for el in soup.find_all(class_ = 'fc-item__content')]\n",
    "    print('\\n{}:'.format(topic))\n",
    "    i = 0\n",
    "    while article_topics.count(topic) < articles_per_topic:\n",
    "        soup = BeautifulSoup(requests.get(url_articles[i]).text, 'html.parser')\n",
    "        try:\n",
    "            title = soup.find(class_ = 'content__headline').text.strip('\\n')\n",
    "            content = ' '.join([el.text for el in soup.find(class_ = 'content__article-body from-content-api js-article__body').find_all('p')])\n",
    "            i += 1\n",
    "            if i == len(url_articles):\n",
    "                print('Only {} articles found in \\\"{}\"'.format(article_topics.count(topic),topic))\n",
    "                break\n",
    "            if title not in article_titles:\n",
    "                article_titles += [title]\n",
    "                article_contents += [content]\n",
    "                article_topics += [topic]\n",
    "                save_to_txt('title-{}'.format(n),title)\n",
    "                save_to_txt('article-{}'.format(n),content)\n",
    "                save_to_txt('topic-{}'.format(n),topic)\n",
    "                print('{}'.format(title))\n",
    "                n += 1\n",
    "                if round(len(article_titles)/10) == len(article_titles)/10:\n",
    "                    print('Article count: {}'.format(len(article_titles)))\n",
    "        except:\n",
    "            i += 1\n",
    "            if i == len(url_articles):\n",
    "                print('Only {} articles found in \\\"{}\"'.format(article_topics.count(topic),topic))\n",
    "                break\n",
    "            pass\n",
    "        \n",
    "                \n",
    "df = pd.DataFrame({'topic':article_topics,'title':article_titles,'content':article_contents})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importación de la base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos la base de datos guardada en carpeta deseada, podemos usar el código del caso de estudio para importar la información."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:01:08.551501Z",
     "start_time": "2020-05-29T17:01:08.529041Z"
    }
   },
   "outputs": [],
   "source": [
    "#número total de artículos para procesar.\n",
    "N = 80\n",
    "#para almacenar los temas, títulos y contenidos de las noticias:\n",
    "topics_array = []\n",
    "titles_array = []\n",
    "corpus = []\n",
    "for i in range(1, N+1):\n",
    "    #consiga el contenido del artículo.\n",
    "    with open('Data/article-' + str(i) + '.txt', 'r') as myfile:\n",
    "        d1=myfile.read().replace('\\n', '')\n",
    "        d1 = d1.lower()\n",
    "        corpus.append(d1)\n",
    "    #consiga el tema original del artículo.\n",
    "    with open('Data/topic-' + str(i) + '.txt', 'r') as myfile:\n",
    "        to1=myfile.read().replace('\\n', '')\n",
    "        to1 = to1.lower()\n",
    "        topics_array.append(to1)\n",
    "    #consiga el título del artículo.\n",
    "    with open('Data/title-' + str(i) + '.txt', 'r') as myfile:\n",
    "        ti1=myfile.read().replace('\\n', '')\n",
    "        ti1 = ti1.lower()\n",
    "        titles_array.append(ti1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación de atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para generar los atributos de cada instancia (artículo):\n",
    "\n",
    "1. Enlazamos todos los corpus de texto de artículos para determinar todas las palabras únicas que se utilizan en el conjunto de datos.\n",
    "2. Buscamos el subconjunto de las entidades del modelo NER que se encuentra entre las palabras únicas que se utilizan en el conjunto de datos (determinado en el paso 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:01:09.741152Z",
     "start_time": "2020-05-29T17:01:08.555024Z"
    }
   },
   "outputs": [],
   "source": [
    "#vector de subconjunto de entidades\n",
    "entity_text_array = [] \n",
    "for i in range(1, N+1):\n",
    "    #cargue el archivo de texto con el contenido del artículo y conviértalo en una lista de palabras\n",
    "    tokens = tokenize(load_entire_file(('Data/article-' + str(i) + '.txt')))\n",
    "    #extraiga todas las entidades conocidas del modelo ner mencionado en este artículo\n",
    "    entities = ner.extract_entities(tokens)\n",
    "    #extraiga las palabras de entidades reales y agréguelas al vector\n",
    "    for e in entities: \n",
    "        range_array = e[0]\n",
    "        tag = e[1]\n",
    "        score = e[2]\n",
    "        score_text = \"{:0.3f}\".format(score)\n",
    "        entity_text = \" \".join(str(tokens[j]) for j in range_array) \n",
    "        entity_text_array.append(entity_text.lower())\n",
    "#elimine las entidades duplicadas que se hayan detectado\n",
    "#entity_text_array = np.unique(entity_text_array)\n",
    "entity_text_array = list(set(entity_text_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T19:12:49.108073Z",
     "start_time": "2020-05-22T19:12:49.094235Z"
    }
   },
   "source": [
    "Ahora que ya tenemos la lista de todas las entidades utilizadas en la base de datos, podemos representar cada artículo como un vector que contiene la puntuación de [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf) para cada entidad almacenada en el `entity_text_array`. Esta tarea se puede realizar fácilmente con la librería [scikit-learn](http://scikit- learn.org/stable/) de Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:01:09.782094Z",
     "start_time": "2020-05-29T17:01:09.742773Z"
    }
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word',\n",
    "                       stop_words='english', vocabulary=entity_text_array)\n",
    "corpus_tf_idf = vect.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos los artículos representados por sus atributos (puntuaciones de TF-IDF), podemos llevar a cabo el agrupamiento espectral de los mismos usando la librería `scikit-learn` de nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:01:09.842068Z",
     "start_time": "2020-05-29T17:01:09.783871Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpectralClustering(affinity='nearest_neighbors', assign_labels='kmeans',\n",
       "                   coef0=1, degree=3, eigen_solver='arpack', eigen_tol=0.0,\n",
       "                   gamma=1.0, kernel_params=None, n_clusters=8,\n",
       "                   n_components=None, n_init=10, n_jobs=None, n_neighbors=10,\n",
       "                   random_state=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change n_clusters to equal the number of clusters desired\n",
    "n_clusters = 8\n",
    "#spectral clustering\n",
    "spectral = cluster.SpectralClustering(n_clusters= n_clusters, \n",
    "                                      eigen_solver='arpack', \n",
    "                                      affinity=\"nearest_neighbors\", \n",
    "                                      n_neighbors = 10)\n",
    "spectral.fit(corpus_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente las siguientes líneas de código permiten ver el output en el siguiente formato (una línea por artículo):\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>__no. artículo, tema, cluster, título__</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:01:09.853066Z",
     "start_time": "2020-05-29T17:01:09.845187Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 elections_2020 7 biden sets solemn tone as trump waits 15 hours to mark covid-19 milestone\n",
      "1 elections_2020 1 revealed: conservative group fighting to restrict voting tied to powerful dark money network\n",
      "2 elections_2020 7 twitter labels trump's false claims with warning for first time\n",
      "3 elections_2020 0 'feels good to be out of my house': biden lays memorial day wreath in delaware\n",
      "4 elections_2020 0 why is trump so restrained about the biden sexual assault allegation?\n",
      "5 elections_2020 1 swing states become partisan battlegrounds in america's fight against covid-19\n",
      "6 elections_2020 4 trump campaign focuses fire on biden as pandemic undermines strategy\n",
      "7 elections_2020 0 'obamagate': fox news focuses on conspiracy theory rather than covid-19\n",
      "8 elections_2020 7 ‘the united states is broken as hell’ – the division in politics over race and class\n",
      "9 elections_2020 7 socialism used to be a dirty word. is america now ready to embrace it?\n",
      "10 world 0 global report: south korea postpones school reopening due to new outbreak\n",
      "11 world 1 'gross incompentence at highest levels': ex-obama adviser blasts trump's covid response\n",
      "12 world 0 monkeys steal covid-19 test samples from health worker in india\n",
      "13 world 0 china threatens 'countermeasures' against uk over hong kong crisis\n",
      "14 world 4 italy considers charges over malta's 'shocking' refusal to rescue migrants\n",
      "15 world 7 eu's greenhouse gas emissions continue to fall as coal ditched\n",
      "16 world 0 police smash people-smuggling ring 'linked to essex lorry deaths'\n",
      "17 world 1 renault to cut 14,600 jobs as part of €2bn cost-saving plan\n",
      "18 world 0 ethiopia's security forces accused of torture, evictions and killings – report\n",
      "19 world 1 dominic cummings cutout appears at rugby league match in australia\n",
      "20 environment 0 new trump public land rules will let alaska hunters kill bear cubs in dens\n",
      "21 environment 6 us lets corporations delay paying environmental fines amid pandemic\n",
      "22 environment 0 we are eating shrimp in record numbers. but for how much longer?\n",
      "23 environment 3 race, wealth and public spaces: us beaches are a new flashpoint of the lockdown\n",
      "24 environment 3 thousands of run-down us dams would kill people if they failed, study finds\n",
      "25 environment 1 when the '500-year flood' hit michigan, residents had to weigh risk of escape in a pandemic\n",
      "26 environment 4 firms ignoring climate crisis will go bankrupt, says mark carney\n",
      "27 environment 4 large heath butterflies return to manchester after 150 years\n",
      "28 environment 4 india's first 'green' village adapts to life without tourists\n",
      "29 environment 1 'this makes chinese medicine look bad': tcm supporters condemn illegal wildlife trade\n",
      "30 soccer 0 liverpool and everton to oppose plans to play games at neutral venues\n",
      "31 soccer 1 slavisa jokanovic: 'euro 92 was taken away from us. we were better than denmark'\n",
      "32 soccer 0 what, when, where? questions answered on premier league's return\n",
      "33 soccer 0 who wouldn’t want to spend a second consecutive summer day indoors?\n",
      "34 soccer 1 golden goal: dietmar hamann for germany v england (2000)\n",
      "35 soccer 0 'people say you shouldn’t do this': nathan jones heads into fire at luton\n",
      "36 soccer 1 'i dance to joe wicks every morning': football commentators in lockdown\n",
      "37 soccer 0 germany rallied to save women's football – the fa should be embarrassed\n",
      "38 soccer 6 premier league plans restart on 17 june with manchester city v arsenal\n",
      "39 soccer 2 barcelona keen on miralem pjanic and refuse to give up on lautaro martínez\n",
      "40 us_politics 2 the minnesota paradox: how race divides prosperous minneapolis\n",
      "41 us_politics 2 'trump knows nothing of minneapolis': mayor hits back at president's tweets\n",
      "42 us_politics 3 trump signs executive order to narrow protections for social media platforms\n",
      "43 us_politics 2 we shouldn't have to witness george floyd's killing for it to spark outrage\n",
      "44 us_politics 4 'here's a bedsheet, make a parachute!' republicans say, pushing us out of a plane\n",
      "45 us_politics 0 why twitter should ban donald trump\n",
      "46 us_politics 0 trump's devoted new press secretary is no different from her predecessors\n",
      "47 us_politics 7 'you have to respond forcefully': can joe biden fight trump's brutal tactics?\n",
      "48 us_politics 5 'we're expendable': black americans pay the price as states lift lockdowns\n",
      "49 business 7 us job losses pass 40m as coronavirus crisis sees claims rise 2.1m in a week\n",
      "50 business 0 food-waste firm bags oprah winfrey and katy perry as investors\n",
      "51 business 0 lufthansa agrees €9bn bailout with german government\n",
      "52 business 0 tackling the dark side of the movie business: the blockchain startup ensuring film workers get paid\n",
      "53 business 0 pizzas in the post: shopify challenges amazon for slice of lockdown trade\n",
      "54 business 0 extra $600 in jobless pay offers many a lifeline – but will it be renewed?\n",
      "55 business 0 exclusive: big pharma rejected eu plan to fast-track vaccines in 2017\n",
      "56 business 0 jobless america: the coronavirus unemployment crisis in figures\n",
      "57 business 0 tiktok's first auteur: zach king on his madcap micro movies\n",
      "58 business 0 the zoom boom: how video-calling became a blessing – and a curse\n",
      "59 tech 2 twitter hides donald trump tweet for 'glorifying violence'\n",
      "60 tech 0 zuckerberg says facebook won't be 'arbiters of truth' after trump threat\n",
      "61 tech 0 donald trump's move against twitter factchecking could backfire\n",
      "62 tech 7 new vulnerability allows users to 'jailbreak' iphones\n",
      "63 tech 0 the chaser goes viral with provocative post mocking zuckerberg’s position on facebook factchecking\n",
      "64 tech 0 work from home song peaks again as listening alters under lockdown\n",
      "65 tech 3 canada court ruling allows us extradition case of huawei executive to proceed\n",
      "66 tech 2 huge rise in hacking attacks on home workers during lockdown\n",
      "67 tech 1 easyjet reveals cyber-attack exposed 9m customers' details\n",
      "68 tech 4 zoom hacker streams child sex abuse footage to plymouth children\n",
      "69 science 0 virgin orbit looks into cause of launcherone test failure\n",
      "70 science 3 easing the lockdown: how will we know if infections bounce back?\n",
      "71 science 0 nhs to increase number of covid-19 patients receiving antibody therapy\n",
      "72 science 7 questions raised over hydroxychloroquine study which caused who to halt trials for covid-19\n",
      "73 science 7 allosaurus dinosaur suspected to be scavenging cannibal\n",
      "74 science 1 hydroxychloroquine and coronavirus: a guide to the scientific studies so far\n",
      "75 science 0 uk minister hails 'game-changing' coronavirus immunity test\n",
      "76 science 0 italian doctors find link between covid-19 and inflammatory disorder\n",
      "77 science 2 manchester's pubs pine for the end of lockdown\n",
      "78 science 0 coronavirus in england: half of those with symptoms not isolating\n",
      "79 science 0 elon musk’s spacex to launch first astronauts from us soil since 2011\n"
     ]
    }
   ],
   "source": [
    "if hasattr(spectral, 'labels_'):\n",
    "    cluster_assignments = spectral.labels_.astype(np.int)\n",
    "    for i in range(0, len(cluster_assignments)):\n",
    "        print (i, topics_array[i], cluster_assignments [i], titles_array[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede observarse, el algoritmo no clasifica los artículos según las secciones de las que se han obtenido. Puede indagar más a fondo en los parámetros del modelo para mejorar dichos resultados, o buscar una explicación para entender el criterio con el que el algoritmo está agrupando los artículos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Created by Iñigo de la Maza. Contact: [idelamaza.com](https://idelamaza.github.io/)\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T23:01:23.545014Z",
     "start_time": "2020-05-30T23:01:23.533756Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of packages and versions:\n",
      "\n",
      "scikit-learn==0.22.1\n",
      "requests==2.23.0\n",
      "pandas==1.0.0\n",
      "numpy==1.18.1\n",
      "mitie==0.7.36\n",
      "matplotlib==3.1.3\n",
      "beautifulsoup4==4.9.1\n"
     ]
    }
   ],
   "source": [
    "## IGNORE THE CODE BELOW ##\n",
    "\n",
    "#Getting names of imported libraries and versions for creating a requirements.txt file\n",
    "import pkg_resources\n",
    "import types\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "            \n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "            \n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "requirements.append(('beautifulsoup4', '4.9.1'))    \n",
    "#Getting the packages already included in requirements.txt\n",
    "with open(r\"../../requirements.txt\", \"r\") as f:\n",
    "    pkgs = [pkg.split('==')[0] for pkg in f.readlines()]\n",
    "#Adding missing packages\n",
    "print('List of packages and versions:\\n')     \n",
    "with open(r\"../../requirements.txt\", \"a\") as f:\n",
    "    for r in requirements:\n",
    "        print(\"{}=={}\".format(*r))\n",
    "        if r[0] not in pkgs:\n",
    "            f.write(\"{}=={}\\n\".format(*r))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.545px",
    "left": "938.091px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
