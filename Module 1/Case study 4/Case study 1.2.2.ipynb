{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Case-study-1.2.2:-Spectral-Clustering---Grouping-News-Stories\" data-toc-modified-id=\"Case-study-1.2.2:-Spectral-Clustering---Grouping-News-Stories-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Case study 1.2.2: Spectral Clustering - Grouping News Stories</a></span></li><li><span><a href=\"#Database-generation-(Web-Scraping)\" data-toc-modified-id=\"Database-generation-(Web-Scraping)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Database generation (Web Scraping)</a></span></li><li><span><a href=\"#Importing-database\" data-toc-modified-id=\"Importing-database-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Importing database</a></span></li><li><span><a href=\"#Feature-generation\" data-toc-modified-id=\"Feature-generation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature generation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study 1.2.2: Spectral Clustering - Grouping News Stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "\n",
    "This case study considers a database of news articles covering different topics, and uses _spectral clustering_ to cluster them depending on the frequency of certain words. The code for generating the database of news articles is provided, but a sample dataset of articles generated on May 28, 2020 from the newspaper The Guardian can be found in the folder `/Data`. This dataset has been generated by using data mining techniques (_web scraping_).\n",
    "\n",
    "This case study uses the NLP library [`mitie`](https://github.com/mit-nlp/MITIE), developed at MIT. All the steps in order to install both the library and the NER model used in this particular case study can be found in the documentation of the library.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T21:55:02.733150Z",
     "start_time": "2020-05-30T21:54:58.439781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading NER model...\n",
      "\n",
      "Tags output by this NER model: ['PERSON', 'LOCATION', 'ORGANIZATION', 'MISC']\n",
      "List of packages and versions:\n",
      "\n",
      "scikit-learn==0.22.1\n",
      "requests==2.23.0\n",
      "pandas==1.0.0\n",
      "numpy==1.18.1\n",
      "mitie==0.7.36\n",
      "matplotlib==3.1.3\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import csv\n",
    "\n",
    "#ML\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import cluster\n",
    "\n",
    "#Web scraping libraries\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#NLP libraries\n",
    "from mitie import *\n",
    "print(\"loading NER model...\")\n",
    "#ner = named_entity_extractor('.../mitie/models/MITIE-models/english/ner_model.dat') #[PATH TO MITIE LIBRARY]\n",
    "ner = named_entity_extractor('/Users/inigo/opt/anaconda3/lib/python3.7/site-packages/mitie/models/MITIE-models/english/ner_model.dat')\n",
    "print(\"\\nTags output by this NER model:\", ner.get_possible_ner_tags())\n",
    "\n",
    "## IGNORE THE CODE BELOW ##\n",
    "#Getting names of imported libraries and versions for creating a requirements.txt file\n",
    "import pkg_resources\n",
    "import types\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            # Split ensures you get root package, \n",
    "            # not just imported function\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name != \"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "print('List of packages and versions:\\n')      \n",
    "\n",
    "for r in requirements:\n",
    "        print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-30T21:57:26.355590Z",
     "start_time": "2020-05-30T21:57:26.345693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zipp\n",
      "zict\n",
      "yfinance\n",
      "yapf\n",
      "yahoofinancials\n",
      "xlwt\n",
      "xlwings\n",
      "XlsxWriter\n",
      "xlrd\n",
      "xgboost\n",
      "wurlitzer\n",
      "wrapt\n",
      "widgetsnbextension\n",
      "wheel\n",
      "Werkzeug\n",
      "webencodings\n",
      "wcwidth\n",
      "watchdog\n",
      "w3lib\n",
      "vincent\n",
      "urllib3\n",
      "unicodecsv\n",
      "ujson\n",
      "typing-extensions\n",
      "typed-ast\n",
      "traitlets\n",
      "tqdm\n",
      "tornado\n",
      "toolz\n",
      "toml\n",
      "testpath\n",
      "terminado\n",
      "termcolor\n",
      "tensorflow\n",
      "tensorflow-estimator\n",
      "tensorboard\n",
      "tblib\n",
      "tabulate\n",
      "tables\n",
      "sympy\n",
      "statsmodels\n",
      "sqlparse\n",
      "SQLAlchemy\n",
      "spyder\n",
      "spyder-kernels\n",
      "sphinxcontrib-websupport\n",
      "sphinxcontrib-serializinghtml\n",
      "sphinxcontrib-qthelp\n",
      "sphinxcontrib-jsmath\n",
      "sphinxcontrib-htmlhelp\n",
      "sphinxcontrib-devhelp\n",
      "sphinxcontrib-applehelp\n",
      "Sphinx\n",
      "SPARQLWrapper\n",
      "soupsieve\n",
      "sortedcontainers\n",
      "sortedcollections\n",
      "snowballstemmer\n",
      "six\n",
      "singledispatch\n",
      "simplegeneric\n",
      "Shapely\n",
      "shap\n",
      "setuptools\n",
      "Send2Trash\n",
      "selenium\n",
      "seaborn\n",
      "scipy\n",
      "scikit-learn\n",
      "scikit-image\n",
      "ruamel-yaml\n",
      "Rtree\n",
      "rope\n",
      "retrying\n",
      "requests\n",
      "regex\n",
      "rdflib\n",
      "ratelim\n",
      "QtPy\n",
      "qtconsole\n",
      "QtAwesome\n",
      "QDarkStyle\n",
      "pyzmq\n",
      "PyYAML\n",
      "PyWavelets\n",
      "pytz\n",
      "pytrends\n",
      "python-language-server\n",
      "python-jsonrpc-server\n",
      "python-dateutil\n",
      "pytest\n",
      "pytest-remotedata\n",
      "pytest-openfiles\n",
      "pytest-doctestplus\n",
      "pytest-astropy\n",
      "pytest-astropy-header\n",
      "pytest-arraydiff\n",
      "PySocks\n",
      "pysb\n",
      "pyrsistent\n",
      "pyproj\n",
      "pyparsing\n",
      "pyOpenSSL\n",
      "pyodbc\n",
      "pylint\n",
      "Pygments\n",
      "pyglet\n",
      "pyflakes\n",
      "pydocstyle\n",
      "pycurl\n",
      "pycrypto\n",
      "pycparser\n",
      "pycosat\n",
      "pycodestyle\n",
      "py\n",
      "ptyprocess\n",
      "psutil\n",
      "protobuf\n",
      "prompt-toolkit\n",
      "prometheus-client\n",
      "prettytable\n",
      "ply\n",
      "pluggy\n",
      "plotly\n",
      "pkginfo\n",
      "pip\n",
      "Pillow\n",
      "pickleshare\n",
      "pexpect\n",
      "pep8\n",
      "patsy\n",
      "pathtools\n",
      "pathlib2\n",
      "path\n",
      "partd\n",
      "parso\n",
      "pandocfilters\n",
      "pandas\n",
      "pandas-datareader\n",
      "packaging\n",
      "OWSLib\n",
      "orca\n",
      "opt-einsum\n",
      "openpyxl\n",
      "opencv-python\n",
      "olefile\n",
      "numpydoc\n",
      "numpy\n",
      "numexpr\n",
      "numba\n",
      "notebook\n",
      "nose\n",
      "nltk\n",
      "networkx\n",
      "nbformat\n",
      "nbconvert\n",
      "navigator-updater\n",
      "multitasking\n",
      "multipledispatch\n",
      "msgpack\n",
      "mpmath\n",
      "more-itertools\n",
      "mock\n",
      "mkl-service\n",
      "mkl-random\n",
      "mkl-fft\n",
      "mitie\n",
      "mitdeeplearning\n",
      "mistune\n",
      "mccabe\n",
      "matplotlib\n",
      "MarkupSafe\n",
      "Markdown\n",
      "lxml\n",
      "locket\n",
      "llvmlite\n",
      "lightgbm\n",
      "libarchive-c\n",
      "lesscpy\n",
      "lazy-object-proxy\n",
      "kiwisolver\n",
      "keyring\n",
      "Keras-Preprocessing\n",
      "Keras-Applications\n",
      "keepalive\n",
      "jupyterthemes\n",
      "jupyterlab\n",
      "jupyterlab-server\n",
      "jupyter\n",
      "jupyter-nbextensions-configurator\n",
      "jupyter-latex-envs\n",
      "jupyter-highlight-selected-word\n",
      "jupyter-core\n",
      "jupyter-contrib-nbextensions\n",
      "jupyter-contrib-core\n",
      "jupyter-console\n",
      "jupyter-client\n",
      "julia\n",
      "jsonschema\n",
      "json5\n",
      "joblib\n",
      "Jinja2\n",
      "jedi\n",
      "jdcal\n",
      "itsdangerous\n",
      "isort\n",
      "isodate\n",
      "ipywidgets\n",
      "ipython\n",
      "ipython-sql\n",
      "ipython-genutils\n",
      "ipykernel\n",
      "intervaltree\n",
      "importlib-metadata\n",
      "imagesize\n",
      "imageio\n",
      "idna\n",
      "hypothesis\n",
      "html5lib\n",
      "HeapDict\n",
      "h5py\n",
      "gym\n",
      "grpcio\n",
      "greenlet\n",
      "graphviz\n",
      "google-pasta\n",
      "gmpy2\n",
      "glob2\n",
      "gevent\n",
      "geopy\n",
      "geographiclib\n",
      "geocoder\n",
      "gast\n",
      "future\n",
      "fsspec\n",
      "folium\n",
      "Flask\n",
      "flake8\n",
      "filelock\n",
      "fastcache\n",
      "et-xmlfile\n",
      "entrypoints\n",
      "eli5\n",
      "docutils\n",
      "distributed\n",
      "diff-match-patch\n",
      "defusedxml\n",
      "decorator\n",
      "dask\n",
      "cytoolz\n",
      "Cython\n",
      "cycler\n",
      "cryptography\n",
      "contextlib2\n",
      "conda\n",
      "conda-verify\n",
      "conda-package-handling\n",
      "conda-build\n",
      "colorama\n",
      "clyent\n",
      "cloudpickle\n",
      "click\n",
      "chart-studio\n",
      "chardet\n",
      "cffi\n",
      "certifi\n",
      "category-encoders\n",
      "catboost\n",
      "branca\n",
      "Bottleneck\n",
      "boto\n",
      "bokeh\n",
      "bleach\n",
      "bkcharts\n",
      "bitarray\n",
      "beautifulsoup4\n",
      "bayesian-optimization\n",
      "backports.weakref\n",
      "backports.tempfile\n",
      "backports.shutil-get-terminal-size\n",
      "backports.functools-lru-cache\n",
      "backcall\n",
      "Babel\n",
      "autopep8\n",
      "attrs\n",
      "atomicwrites\n",
      "astropy\n",
      "astroid\n",
      "astor\n",
      "asn1crypto\n",
      "argh\n",
      "appnope\n",
      "applaunchservices\n",
      "anaconda-project\n",
      "anaconda-navigator\n",
      "anaconda-client\n",
      "altair\n",
      "alabaster\n",
      "absl-py\n",
      "appscript\n"
     ]
    }
   ],
   "source": [
    "for m in pkg_resources.working_set:\n",
    "    print(m.project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database generation (Web Scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, news articles from the newspaper __The Guardian__ are collected across 8 different topics. The steps that are performed for building the dataset are:\n",
    "\n",
    "1. Retrieving the source code from the main site of The Guardian and storing the links to different sections of interest in a list.\n",
    "2. Iterating through the list of links and getting the information (title, and content) for 10 articles from each topic.\n",
    "3. Storing the articles, titles, and topics in `.txt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:02:34.174438Z",
     "start_time": "2020-05-29T17:02:33.363077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: Elections_2020 (https://www.theguardian.com/us-news/us-elections-2020)\n",
      "Topic 2: World (https://www.theguardian.com/world)\n",
      "Topic 3: Environment (https://www.theguardian.com/us/environment)\n",
      "Topic 4: Soccer (https://www.theguardian.com/football)\n",
      "Topic 5: US_Politics (https://www.theguardian.com/us-news/us-politics)\n",
      "Topic 6: Business (https://www.theguardian.com/us/business)\n",
      "Topic 7: Tech (https://www.theguardian.com/us/technology)\n",
      "Topic 8: Science (https://www.theguardian.com/science)\n"
     ]
    }
   ],
   "source": [
    "UK_news_url = 'https://www.theguardian.com/uk'\n",
    "#Descargando los links de los diferentes temas\n",
    "html_data = requests.get(UK_news_url).text\n",
    "soup = BeautifulSoup(html_data, 'html.parser')\n",
    "url_topics = [el.find('a')['href'] for el in soup.find_all(class_ = 'subnav__item')[1:9]]\n",
    "topics = [el.text.strip('\\n').replace(' ','_') for el in soup.find_all(class_ = 'subnav-link')[1:9]]\n",
    "for i in range(len(topics)):\n",
    "    print('Topic {}: {} ({})'.format(i+1,topics[i],url_topics[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:03:29.800041Z",
     "start_time": "2020-05-29T17:02:34.178052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting news articles from The Guardian: \n",
      "\n",
      "\n",
      "Elections_2020:\n",
      "Revealed: conservative group fighting to restrict voting tied to powerful dark money network\n",
      "Republicans sense rich pickings in Biden archive – but will it be made public?\n",
      "Twitter labels Trump's false claims with warning for first time\n",
      "'Feels good to be out of my house': Biden lays Memorial Day wreath in Delaware\n",
      "'You have to respond forcefully': can Joe Biden fight Trump's brutal tactics?\n",
      "Why is Trump so restrained about the Biden sexual assault allegation?\n",
      "Swing states become partisan battlegrounds in America's fight against Covid-19\n",
      "Trump campaign focuses fire on Biden as pandemic undermines strategy\n",
      "‘The United States is broken as hell’ – the division in politics over race and class\n",
      "Socialism used to be a dirty word. Is America now ready to embrace it?\n",
      "Article count: 10\n",
      "\n",
      "World:\n",
      "Global report: South Korea postpones school reopening due to new outbreak\n",
      "'Gross incompentence at highest levels': ex-Obama adviser blasts Trump's Covid response\n",
      "'Demand is huge': EU citizens flock to open-air cinemas as lockdown eases\n",
      "China threatens 'countermeasures' against UK over Hong Kong crisis\n",
      "Italy considers charges over Malta's 'shocking' refusal to rescue migrants\n",
      "Barrow journalist hounded out of Cumbria for reporting court case\n",
      "University of Queensland student suspended for two years after speaking out on China ties\n",
      "Renault to cut 14,600 jobs as part of €2bn cost-saving plan\n",
      "Ethiopia's security forces accused of torture, evictions and killings – report\n",
      "Dominic Cummings cutout appears at rugby league match in Australia\n",
      "Article count: 20\n",
      "\n",
      "Environment:\n",
      "New Trump public land rules will let Alaska hunters kill bear cubs in dens\n",
      "We are eating shrimp in record numbers. But for how much longer?\n",
      "Race, wealth and public spaces: US beaches are a new flashpoint of the lockdown\n",
      "Thousands of run-down US dams would kill people if they failed, study finds\n",
      "When the '500-year flood' hit Michigan, residents had to weigh risk of escape in a pandemic\n",
      "Firms ignoring climate crisis will go bankrupt, says Mark Carney\n",
      "Large heath butterflies return to Manchester after 150 years\n",
      "India's first 'green' village adapts to life without tourists\n",
      "'This makes Chinese medicine look bad': TCM supporters condemn illegal wildlife trade\n",
      "'We've never seen this': wildlife thrives in closed US national parks\n",
      "Article count: 30\n",
      "\n",
      "Soccer:\n",
      "Liverpool and Everton to oppose plans to play games at neutral venues\n",
      "Slavisa Jokanovic: 'Euro 92 was taken away from us. We were better than Denmark'\n",
      "What, when, where? Questions answered on Premier League's return\n",
      "Who wouldn’t want to spend a second consecutive summer day indoors?\n",
      "Golden Goal: Dietmar Hamann for Germany v England (2000)\n",
      "'People say you shouldn’t do this': Nathan Jones heads into fire at Luton\n",
      "Germany rallied to save women's football – the FA should be embarrassed\n",
      "Premier League plans restart on 17 June with Manchester City v Arsenal\n",
      "Barcelona keen on Miralem Pjanic and refuse to give up on Lautaro Martínez\n",
      "Serie A gets the go-ahead to resume in just over three weeks\n",
      "Article count: 40\n",
      "\n",
      "US_Politics:\n",
      "Move on from lies and arrogance? No, let’s reflect\n",
      "The Minnesota paradox: how race divides prosperous Minneapolis\n",
      "'Trump knows nothing of Minneapolis': mayor hits back at president's tweets\n",
      "We shouldn't have to witness George Floyd's killing for it to spark outrage\n",
      "'Here's a bedsheet, make a parachute!' Republicans say, pushing us out of a plane\n",
      "Trump's devoted new press secretary is no different from her predecessors\n",
      "'We're expendable': black Americans pay the price as states lift lockdowns\n",
      "Only 7 articles found in \"US_Politics\"\n",
      "\n",
      "Business:\n",
      "US job losses pass 40m as coronavirus crisis sees claims rise 2.1m in a week\n",
      "'I don’t know how I’ll survive': the laid-off workers devastated by coronavirus cuts\n",
      "Pizzas in the post: Shopify challenges Amazon for slice of lockdown trade\n",
      "Article count: 50\n",
      "Extra $600 in jobless pay offers many a lifeline – but will it be renewed?\n",
      "Exclusive: big pharma rejected EU plan to fast-track vaccines in 2017\n",
      "Jobless America: the coronavirus unemployment crisis in figures\n",
      "TikTok's first auteur: Zach King on his madcap micro movies\n",
      "The Zoom boom: how video-calling became a blessing – and a curse\n",
      "The Navajo teenager who went viral reporting on coronavirus: 'I just want us to be seen'\n",
      "The worst is over, but recovery for the UK economy will take years\n",
      "\n",
      "Tech:\n",
      "Twitter hides Donald Trump tweet for 'glorifying violence'\n",
      "Zuckerberg says Facebook won't be 'arbiters of truth' after Trump threat\n",
      "Garmin Forerunner 245 Music review: a runner’s best friend\n",
      "Article count: 60\n",
      "YouTube investigates automatic deletion of comments criticising China Communist party\n",
      "New vulnerability allows users to 'jailbreak' iPhones\n",
      "The Chaser goes viral with provocative post mocking Zuckerberg’s position on Facebook factchecking\n",
      "Work From Home song peaks again as listening alters under lockdown\n",
      "Why Twitter should ban Donald Trump\n",
      "Huge rise in hacking attacks on home workers during lockdown\n",
      "EasyJet reveals cyber-attack exposed 9m customers' details\n",
      "\n",
      "Science:\n",
      "Virgin Orbit looks into cause of LauncherOne test failure\n",
      "Easing the lockdown: how will we know if infections bounce back?\n",
      "NHS to increase number of Covid-19 patients receiving antibody therapy\n",
      "Article count: 70\n",
      "Questions raised over hydroxychloroquine study which caused WHO to halt trials for Covid-19\n",
      "Allosaurus dinosaur suspected to be scavenging cannibal\n",
      "Hydroxychloroquine and coronavirus: a guide to the scientific studies so far\n",
      "UK minister hails 'game-changing' coronavirus immunity test\n",
      "Italian doctors find link between Covid-19 and inflammatory disorder\n",
      "Cut air pollution to help avoid second coronavirus peak, MPs urge\n",
      "Coronavirus in England: half of those with symptoms not isolating\n"
     ]
    }
   ],
   "source": [
    "def save_to_txt(filename, content):\n",
    "    '''\n",
    "    Creates a new .txt file with as specific name in the Data directory\n",
    "    '''\n",
    "    with open(r\"Data/{}.txt\".format(filename), \"w\") as f:\n",
    "        print(content, file=f)\n",
    "\n",
    "article_titles = []\n",
    "article_contents = []\n",
    "article_topics = []\n",
    "articles_per_topic = 10\n",
    "print('Getting news articles from The Guardian: \\n')\n",
    "n = 1\n",
    "for topic, url_topic in list(zip(topics,url_topics)):\n",
    "    soup = BeautifulSoup(requests.get(url_topic).text, 'html.parser')\n",
    "    url_articles = [el.find('a')['href'] for el in soup.find_all(class_ = 'fc-item__content')]\n",
    "    print('\\n{}:'.format(topic))\n",
    "    i = 0\n",
    "    while article_topics.count(topic) < articles_per_topic:\n",
    "        soup = BeautifulSoup(requests.get(url_articles[i]).text, 'html.parser')\n",
    "        try:\n",
    "            title = soup.find(class_ = 'content__headline').text.strip('\\n')\n",
    "            content = ' '.join([el.text for el in soup.find(class_ = 'content__article-body from-content-api js-article__body').find_all('p')])\n",
    "            i += 1\n",
    "            if i == len(url_articles):\n",
    "                print('Only {} articles found in \\\"{}\"'.format(article_topics.count(topic),topic))\n",
    "                break\n",
    "            if title not in article_titles:\n",
    "                article_titles += [title]\n",
    "                article_contents += [content]\n",
    "                article_topics += [topic]\n",
    "                save_to_txt('title-{}'.format(n),title)\n",
    "                save_to_txt('article-{}'.format(n),content)\n",
    "                save_to_txt('topic-{}'.format(n),topic)\n",
    "                print('{}'.format(title))\n",
    "                n += 1\n",
    "                if round(len(article_titles)/10) == len(article_titles)/10:\n",
    "                    print('Article count: {}'.format(len(article_titles)))\n",
    "        except:\n",
    "            i += 1\n",
    "            if i == len(url_articles):\n",
    "                print('Only {} articles found in \\\"{}\"'.format(article_topics.count(topic),topic))\n",
    "                break\n",
    "            pass\n",
    "        \n",
    "                \n",
    "df = pd.DataFrame({'topic':article_topics,'title':article_titles,'content':article_contents})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the database stored in the directory we want, we can use the code provided in the case study to import the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:03:29.825852Z",
     "start_time": "2020-05-29T17:03:29.801703Z"
    }
   },
   "outputs": [],
   "source": [
    "#total number of articles to process\n",
    "N = 80\n",
    "#in memory stores for the topics, titles and contents of the news stories\n",
    "topics_array = []\n",
    "titles_array = []\n",
    "corpus = []\n",
    "for i in range(1, N+1):\n",
    "    #get the contents of the article\n",
    "    with open('Data/article-' + str(i) + '.txt', 'r') as myfile:\n",
    "        d1=myfile.read().replace('\\n', '')\n",
    "        d1 = d1.lower()\n",
    "        corpus.append(d1)\n",
    "    #get the original topic of the article\n",
    "    with open('Data/topic-' + str(i) + '.txt', 'r') as myfile:\n",
    "        to1=myfile.read().replace('\\n', '')\n",
    "        to1 = to1.lower()\n",
    "        topics_array.append(to1)\n",
    "    #get the title of the article\n",
    "    with open('Data/title-' + str(i) + '.txt', 'r') as myfile:\n",
    "        ti1=myfile.read().replace('\\n', '')\n",
    "        ti1 = ti1.lower()\n",
    "        titles_array.append(ti1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to do the following:\n",
    "1. Loop over all the article text corpuses to determine all the unique words used across our dataset.\n",
    "2. Find the subset of the entities from the NER model that are among the unique words being used across the dataset (determined in step 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:03:31.043068Z",
     "start_time": "2020-05-29T17:03:29.827100Z"
    }
   },
   "outputs": [],
   "source": [
    "#entity subset array\n",
    "entity_text_array = [] \n",
    "for i in range(1, N+1):\n",
    "    #load the article contents text file and convert it into a list of words.\n",
    "    tokens = tokenize(load_entire_file(('Data/article-' + str(i) + '.txt')))\n",
    "    #extract all entities known to the ner model mentioned in this article\n",
    "    entities = ner.extract_entities(tokens)\n",
    "    #extract the actual entity words and append to the array\n",
    "    for e in entities: \n",
    "        range_array = e[0]\n",
    "        tag = e[1]\n",
    "        score = e[2]\n",
    "        score_text = \"{:0.3f}\".format(score)\n",
    "        entity_text = \" \".join(str(tokens[j]) for j in range_array) \n",
    "        entity_text_array.append(entity_text.lower())\n",
    "#remove duplicate entities detected\n",
    "#entity_text_array = np.unique(entity_text_array)\n",
    "entity_text_array = list(set(entity_text_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T19:12:49.108073Z",
     "start_time": "2020-05-22T19:12:49.094235Z"
    }
   },
   "source": [
    "Now that we have the list of all entities used across our dataset, we can represent each article as a vector that contains the [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf ) score for each entity stored in the `entity_text_array`. This task can easily be achieved by using the [scikit-learn library](http://scikitlearn.org/stable/) for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:03:31.082928Z",
     "start_time": "2020-05-29T17:03:31.044843Z"
    }
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word',\n",
    "                       stop_words='english', vocabulary=entity_text_array)\n",
    "corpus_tf_idf = vect.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the articles represented as vectors of their TF-IDF scores, we are ready to perform Spectral Clustering on the articles. We can use the scikit-learn library for this purpose as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:03:31.139659Z",
     "start_time": "2020-05-29T17:03:31.085448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpectralClustering(affinity='nearest_neighbors', assign_labels='kmeans',\n",
       "                   coef0=1, degree=3, eigen_solver='arpack', eigen_tol=0.0,\n",
       "                   gamma=1.0, kernel_params=None, n_clusters=8,\n",
       "                   n_components=None, n_init=10, n_jobs=None, n_neighbors=10,\n",
       "                   random_state=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change n_clusters to equal the number of clusters desired\n",
    "n_clusters = 8\n",
    "#spectral clustering\n",
    "spectral = cluster.SpectralClustering(n_clusters= n_clusters, \n",
    "                                      eigen_solver='arpack', \n",
    "                                      affinity=\"nearest_neighbors\", \n",
    "                                      n_neighbors = 10)\n",
    "spectral.fit(corpus_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the spectral clustering model fitted to the dataset. The following lines of code will help us see the output in the following format (one line per article):\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>__article_number, topic, spectral_clustering_cluster_number, article_title__</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-29T17:03:31.152178Z",
     "start_time": "2020-05-29T17:03:31.143422Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 elections_2020 0 revealed: conservative group fighting to restrict voting tied to powerful dark money network\n",
      "1 elections_2020 7 republicans sense rich pickings in biden archive – but will it be made public?\n",
      "2 elections_2020 7 twitter labels trump's false claims with warning for first time\n",
      "3 elections_2020 5 'feels good to be out of my house': biden lays memorial day wreath in delaware\n",
      "4 elections_2020 0 'you have to respond forcefully': can joe biden fight trump's brutal tactics?\n",
      "5 elections_2020 0 why is trump so restrained about the biden sexual assault allegation?\n",
      "6 elections_2020 4 swing states become partisan battlegrounds in america's fight against covid-19\n",
      "7 elections_2020 0 trump campaign focuses fire on biden as pandemic undermines strategy\n",
      "8 elections_2020 4 ‘the united states is broken as hell’ – the division in politics over race and class\n",
      "9 elections_2020 0 socialism used to be a dirty word. is america now ready to embrace it?\n",
      "10 world 4 global report: south korea postpones school reopening due to new outbreak\n",
      "11 world 0 'gross incompentence at highest levels': ex-obama adviser blasts trump's covid response\n",
      "12 world 1 'demand is huge': eu citizens flock to open-air cinemas as lockdown eases\n",
      "13 world 7 china threatens 'countermeasures' against uk over hong kong crisis\n",
      "14 world 3 italy considers charges over malta's 'shocking' refusal to rescue migrants\n",
      "15 world 0 barrow journalist hounded out of cumbria for reporting court case\n",
      "16 world 7 university of queensland student suspended for two years after speaking out on china ties\n",
      "17 world 3 renault to cut 14,600 jobs as part of €2bn cost-saving plan\n",
      "18 world 3 ethiopia's security forces accused of torture, evictions and killings – report\n",
      "19 world 5 dominic cummings cutout appears at rugby league match in australia\n",
      "20 environment 0 new trump public land rules will let alaska hunters kill bear cubs in dens\n",
      "21 environment 0 we are eating shrimp in record numbers. but for how much longer?\n",
      "22 environment 4 race, wealth and public spaces: us beaches are a new flashpoint of the lockdown\n",
      "23 environment 0 thousands of run-down us dams would kill people if they failed, study finds\n",
      "24 environment 0 when the '500-year flood' hit michigan, residents had to weigh risk of escape in a pandemic\n",
      "25 environment 0 firms ignoring climate crisis will go bankrupt, says mark carney\n",
      "26 environment 3 large heath butterflies return to manchester after 150 years\n",
      "27 environment 0 india's first 'green' village adapts to life without tourists\n",
      "28 environment 0 'this makes chinese medicine look bad': tcm supporters condemn illegal wildlife trade\n",
      "29 environment 0 'we've never seen this': wildlife thrives in closed us national parks\n",
      "30 soccer 0 liverpool and everton to oppose plans to play games at neutral venues\n",
      "31 soccer 0 slavisa jokanovic: 'euro 92 was taken away from us. we were better than denmark'\n",
      "32 soccer 0 what, when, where? questions answered on premier league's return\n",
      "33 soccer 4 who wouldn’t want to spend a second consecutive summer day indoors?\n",
      "34 soccer 0 golden goal: dietmar hamann for germany v england (2000)\n",
      "35 soccer 3 'people say you shouldn’t do this': nathan jones heads into fire at luton\n",
      "36 soccer 0 germany rallied to save women's football – the fa should be embarrassed\n",
      "37 soccer 0 premier league plans restart on 17 june with manchester city v arsenal\n",
      "38 soccer 3 barcelona keen on miralem pjanic and refuse to give up on lautaro martínez\n",
      "39 soccer 0 serie a gets the go-ahead to resume in just over three weeks\n",
      "40 us_politics 7 move on from lies and arrogance? no, let’s reflect\n",
      "41 us_politics 7 the minnesota paradox: how race divides prosperous minneapolis\n",
      "42 us_politics 0 'trump knows nothing of minneapolis': mayor hits back at president's tweets\n",
      "43 us_politics 0 we shouldn't have to witness george floyd's killing for it to spark outrage\n",
      "44 us_politics 7 'here's a bedsheet, make a parachute!' republicans say, pushing us out of a plane\n",
      "45 us_politics 6 trump's devoted new press secretary is no different from her predecessors\n",
      "46 us_politics 3 'we're expendable': black americans pay the price as states lift lockdowns\n",
      "47 business 3 us job losses pass 40m as coronavirus crisis sees claims rise 2.1m in a week\n",
      "48 business 5 'i don’t know how i’ll survive': the laid-off workers devastated by coronavirus cuts\n",
      "49 business 0 pizzas in the post: shopify challenges amazon for slice of lockdown trade\n",
      "50 business 0 extra $600 in jobless pay offers many a lifeline – but will it be renewed?\n",
      "51 business 0 exclusive: big pharma rejected eu plan to fast-track vaccines in 2017\n",
      "52 business 0 jobless america: the coronavirus unemployment crisis in figures\n",
      "53 business 0 tiktok's first auteur: zach king on his madcap micro movies\n",
      "54 business 0 the zoom boom: how video-calling became a blessing – and a curse\n",
      "55 business 0 the navajo teenager who went viral reporting on coronavirus: 'i just want us to be seen'\n",
      "56 business 0 the worst is over, but recovery for the uk economy will take years\n",
      "57 tech 0 twitter hides donald trump tweet for 'glorifying violence'\n",
      "58 tech 0 zuckerberg says facebook won't be 'arbiters of truth' after trump threat\n",
      "59 tech 3 garmin forerunner 245 music review: a runner’s best friend\n",
      "60 tech 0 youtube investigates automatic deletion of comments criticising china communist party\n",
      "61 tech 6 new vulnerability allows users to 'jailbreak' iphones\n",
      "62 tech 7 the chaser goes viral with provocative post mocking zuckerberg’s position on facebook factchecking\n",
      "63 tech 6 work from home song peaks again as listening alters under lockdown\n",
      "64 tech 5 why twitter should ban donald trump\n",
      "65 tech 3 huge rise in hacking attacks on home workers during lockdown\n",
      "66 tech 7 easyjet reveals cyber-attack exposed 9m customers' details\n",
      "67 science 5 virgin orbit looks into cause of launcherone test failure\n",
      "68 science 3 easing the lockdown: how will we know if infections bounce back?\n",
      "69 science 0 nhs to increase number of covid-19 patients receiving antibody therapy\n",
      "70 science 0 questions raised over hydroxychloroquine study which caused who to halt trials for covid-19\n",
      "71 science 2 allosaurus dinosaur suspected to be scavenging cannibal\n",
      "72 science 7 hydroxychloroquine and coronavirus: a guide to the scientific studies so far\n",
      "73 science 3 uk minister hails 'game-changing' coronavirus immunity test\n",
      "74 science 0 italian doctors find link between covid-19 and inflammatory disorder\n",
      "75 science 3 cut air pollution to help avoid second coronavirus peak, mps urge\n",
      "76 science 0 coronavirus in england: half of those with symptoms not isolating\n",
      "77 science 6 manchester's pubs pine for the end of lockdown\n",
      "78 science 3 coronavirus in england: half of those with symptoms not isolating\n",
      "79 science 0 elon musk’s spacex to launch first astronauts from us soil since 2011\n"
     ]
    }
   ],
   "source": [
    "if hasattr(spectral, 'labels_'):\n",
    "    cluster_assignments = spectral.labels_.astype(np.int)\n",
    "    for i in range(0, len(cluster_assignments)):\n",
    "        print (i, topics_array[i], cluster_assignments [i], titles_array[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be observed, the algorithm does not classify the articles according to the newspaper sections they have been taken from. You can take a further look at the model parameters in order to improve those results, or find insights about the criteria the algorithm is currently using to cluster the articles."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
