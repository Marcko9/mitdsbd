{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Case-study-1.2.2:-Spectral-Clustering---Grouping-News-Stories\" data-toc-modified-id=\"Case-study-1.2.2:-Spectral-Clustering---Grouping-News-Stories-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Case study 1.2.2: Spectral Clustering - Grouping News Stories</a></span></li><li><span><a href=\"#Database-generation-(Web-Scraping)\" data-toc-modified-id=\"Database-generation-(Web-Scraping)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Database generation (Web Scraping)</a></span></li><li><span><a href=\"#Importing-database\" data-toc-modified-id=\"Importing-database-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Importing database</a></span></li><li><span><a href=\"#Feature-generation\" data-toc-modified-id=\"Feature-generation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Feature generation</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study 1.2.2: Spectral Clustering - Grouping News Stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "\n",
    "This case study considers a database of news articles covering different topics, and uses _spectral clustering_ to cluster them depending on the frequency of certain words. The code for generating the database of news articles is provided, but a sample dataset of articles generated on June 8, 2020 from the newspaper The Guardian can be found in the folder `/Data`. This dataset has been generated by using data mining techniques (_web scraping_).\n",
    "\n",
    "This case study uses the NLP library [`mitie`](https://github.com/mit-nlp/MITIE), developed at MIT. All the steps in order to install both the library and the NER model used in this particular case study can be found in the documentation of the library.\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-10T17:10:37.064519Z",
     "start_time": "2020-06-10T17:10:33.399046Z"
    },
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading NER model...\n",
      "\n",
      "Tags output by this NER model: ['PERSON', 'LOCATION', 'ORGANIZATION', 'MISC']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import csv\n",
    "\n",
    "#ML\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import cluster\n",
    "\n",
    "#Web scraping libraries\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#NLP libraries\n",
    "from mitie import *\n",
    "#In order to run this notebook you need to download the NER model from mitie \n",
    "print(\"loading NER model...\")\n",
    "#ner = named_entity_extractor('.../mitie/models/MITIE-models/english/ner_model.dat') #[PATH TO MITIE LIBRARY]\n",
    "ner = named_entity_extractor('/Users/inigo/opt/anaconda3/lib/python3.7/site-packages/mitie/models/MITIE-models/english/ner_model.dat')\n",
    "print(\"\\nTags output by this NER model:\", ner.get_possible_ner_tags())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database generation (Web Scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, news articles from the newspaper __The Guardian__ are collected across 8 different topics. The steps that are performed for building the dataset are:\n",
    "\n",
    "1. Retrieving the source code from the main site of The Guardian and storing the links to different sections of interest in a list.\n",
    "2. Iterating through the list of links and getting the information (title, and content) for 10 articles from each topic.\n",
    "3. Storing the articles, titles, and topics in `.txt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T23:40:29.476817Z",
     "start_time": "2020-06-08T23:40:28.843820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: Elections_2020 (https://www.theguardian.com/us-news/us-elections-2020)\n",
      "Topic 2: World (https://www.theguardian.com/world)\n",
      "Topic 3: Environment (https://www.theguardian.com/us/environment)\n",
      "Topic 4: Soccer (https://www.theguardian.com/football)\n",
      "Topic 5: US_Politics (https://www.theguardian.com/us-news/us-politics)\n",
      "Topic 6: Business (https://www.theguardian.com/us/business)\n",
      "Topic 7: Tech (https://www.theguardian.com/us/technology)\n",
      "Topic 8: Science (https://www.theguardian.com/science)\n"
     ]
    }
   ],
   "source": [
    "UK_news_url = 'https://www.theguardian.com/uk'\n",
    "#Descargando los links de los diferentes temas\n",
    "html_data = requests.get(UK_news_url).text\n",
    "soup = BeautifulSoup(html_data, 'html.parser')\n",
    "url_topics = [el.find('a')['href'] for el in soup.find_all(class_ = 'subnav__item')[1:9]]\n",
    "topics = [el.text.strip('\\n').replace(' ','_') for el in soup.find_all(class_ = 'subnav-link')[1:9]]\n",
    "for i in range(len(topics)):\n",
    "    print('Topic {}: {} ({})'.format(i+1,topics[i],url_topics[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T23:41:14.953254Z",
     "start_time": "2020-06-08T23:40:29.479598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting news articles from The Guardian: \n",
      "\n",
      "\n",
      "Elections_2020:\n",
      "UK diplomats fear end of special relationship if Trump re-elected\n",
      "Colin Powell endorses Joe Biden for US president\n",
      "Joe Biden officially clinches Democratic presidential nomination\n",
      "Joe Biden says '10-15%' of Americans 'are just not very good people'\n",
      "Trump tried to vote with wrong address while railing against voter fraud\n",
      "Democrats unveil ambitious plan for police reform: 'This is a first step'\n",
      "Trump and Biden offer starkly different visions with nation at a crossroads\n",
      "America's seniors ebb away from Trump as coronavirus response disappoints\n",
      "‘It could have a chilling effect’: why Trump is ramping up attacks on mail-in voting\n",
      "'You have to respond forcefully': can Joe Biden fight Trump's brutal tactics?\n",
      "Article count: 10\n",
      "\n",
      "World:\n",
      "New York cautiously starts to reopen for business after coronavirus lockdown\n",
      "Matt Hancock hails coronavirus 'retreat' as UK deaths tumble\n",
      "Workers in Tokyo's red-light district to be tested for coronavirus after new spike\n",
      "Prince Andrew in war of words with US prosecutors over Epstein\n",
      "Trump move to take US troops out of Germany 'a dangerous game'\n",
      "US has officially entered first recession since 2009\n",
      "Labour's left uneasy with leader's view on tearing down Colston statue\n",
      "Sweden to present findings on Olof Palme assassination\n",
      "Alarm at Turkish plan to expand powers of nightwatchmen\n",
      "'It was hell': Spanish cocaine raid adds to shipboard misery for 4,000 cows\n",
      "Article count: 20\n",
      "\n",
      "Environment:\n",
      "$1m treasure in Rocky Mountains has been found, says Forrest Fenn\n",
      "Louisiana: coastal residents evacuated as Tropical Storm Cristobal approaches\n",
      "Trump orders agencies cut environment reviews, citing 'economic emergency'\n",
      "Court overturns EPA approval of popular herbicide made by Monsanto\n",
      "US ranks 24th in the world on environmental performance\n",
      "Renewables surpass coal in US energy generation for first time in 130 years\n",
      "Firms ignoring climate crisis will go bankrupt, says Mark Carney\n",
      "Walking app helps tree lovers know their sycamores from their maples\n",
      "Many of the 300 plants and animals endemic to Canada at risk, report finds\n",
      "Why can't we leave them alone? The troubling truth about selfies with sloths\n",
      "Article count: 30\n",
      "\n",
      "Soccer:\n",
      "Raheem Sterling demands English football gives black managers a chance\n",
      "Premier League restart preview No 3: Bournemouth\n",
      "Billy Kee: 'If I didn’t opt out of football, I don’t know if I would be alive'\n",
      "Merseyside derby in limbo after safety group postpones Goodison decision\n",
      "Bayern remind Leverkusen of just where they sit in the pecking order\n",
      "Premier League restart preview No 2: Aston Villa\n",
      "Premier League restart preview No 1: Arsenal\n",
      "Pep Clotet to step down as Birmingham manager at end of season\n",
      "Premier League clubs turn to online dating methods in the transfer market\n",
      "My favourite game: when Denmark beat Uruguay 6-1 at the 86 World Cup\n",
      "Article count: 40\n",
      "\n",
      "US_Politics:\n",
      "Trump plans to resume election rallies despite warnings over large crowds\n",
      "‘Apathy is no longer a choice’: will the George Floyd protests energize young voters?\n",
      "‘They set us up’: US police arrested over 10,000 protesters, many non-violent\n",
      "Minneapolis lawmakers vow to disband police department in historic move\n",
      "Racism in America is not the exception – it's the norm\n",
      "America's top cop is a rightwing culture warrior who hates disorder. What could go wrong?\n",
      "A week that shook a nation: anger burns as power of protests leaves Trump exposed\n",
      "Iowa touted its Covid-19 testing. Now officials are calling for an investigation\n",
      "'As guarded as Fort Knox': the inside story of Hillary Clinton's presidential campaign\n",
      "'How did we get here?': Trump has normalised mayhem and the US is paying the price\n",
      "Article count: 50\n",
      "\n",
      "Business:\n",
      "BP to cut 10,000 jobs worldwide amid huge drop in demand for oil\n",
      "World Bank warns Covid-19 pandemic risks dramatic rise in poverty\n",
      "Covid-19 vaccine: AstraZeneca has 'approached Gilead over possible merger'\n",
      "Behind the US unemployment figures: five key points from May's jobs report\n",
      "The Cares Act's best-kept secret is a boon not just for survival but for growth\n",
      "Angela Merkel has become the spend, spend, spend chancellor\n",
      "The pandemic exposes US childcare for what it is: ‘a crisis within a crisis’\n",
      "Coronavirus solves one problem for US employers: finding workers\n",
      "Europe’s big two kiss and make up for pandemic rescue deal\n",
      "England could suspend Sunday trading laws in push to boost economy\n",
      "Article count: 60\n",
      "\n",
      "Tech:\n",
      "Facebook moderators join criticism of Zuckerberg over Trump stance\n",
      "Smart appliances may not be worth money in long run, warns Which?\n",
      "Samsung Galaxy Z Flip review: four months with the folding phone\n",
      "Zoom to exclude free calls from end-to-end encryption to allow FBI cooperation\n",
      "Embarrassing teenage posts on Facebook? Now you can delete them\n",
      "More than 140 Zuckerberg-funded scientists call on Facebook to rein in Trump\n",
      "Zuckerberg: Facebook will review policies after backlash over Trump posts\n",
      "Zoom booms as teleconferencing company profits from coronavirus crisis\n",
      "Hackers targeting UK research labs amid vaccine race – GCHQ chief\n",
      "New vulnerability allows users to 'jailbreak' iPhones\n",
      "Article count: 70\n",
      "\n",
      "Science:\n",
      "Gardens of the galaxy: can you grow vegetables on Mars?\n",
      "We can no longer ignore the potential of psychedelic drugs to treat depression\n",
      "Starwatch: how to find the Great Diamond in the sky\n",
      "We often accuse the right of distorting science. But the left changed the coronavirus narrative overnight\n",
      "After six months of coronavirus, how close are we to defeating it?\n",
      "Retracted studies may have damaged public trust in science, top researchers fear\n",
      "The first wave of Covid-19 is not over – but how might a second look?\n",
      "Hydroxychloroquine and coronavirus: a guide to the scientific studies so far\n",
      "UK minister hails 'game-changing' coronavirus immunity test\n",
      "UK ministers face legal challenge for refusal to order PPE inquiry\n",
      "Article count: 80\n"
     ]
    }
   ],
   "source": [
    "def save_to_txt(filename, content):\n",
    "    '''\n",
    "    Creates a new .txt file with as specific name in the Data directory\n",
    "    '''\n",
    "    with open(r\"Data/{}.txt\".format(filename), \"w\") as f:\n",
    "        print(content, file=f)\n",
    "\n",
    "article_titles = []\n",
    "article_contents = []\n",
    "article_topics = []\n",
    "articles_per_topic = 10\n",
    "print('Getting news articles from The Guardian: \\n')\n",
    "n = 1\n",
    "for topic, url_topic in list(zip(topics,url_topics)):\n",
    "    soup = BeautifulSoup(requests.get(url_topic).text, 'html.parser')\n",
    "    url_articles = [el.find('a')['href'] for el in soup.find_all(class_ = 'fc-item__content')]\n",
    "    print('\\n{}:'.format(topic))\n",
    "    i = 0\n",
    "    while article_topics.count(topic) < articles_per_topic:\n",
    "        soup = BeautifulSoup(requests.get(url_articles[i]).text, 'html.parser')\n",
    "        try:\n",
    "            title = soup.find(class_ = 'content__headline').text.strip('\\n')\n",
    "            content = ' '.join([el.text for el in soup.find(class_ = 'content__article-body from-content-api js-article__body').find_all('p')])\n",
    "            i += 1\n",
    "            if i == len(url_articles):\n",
    "                print('Only {} articles found in \\\"{}\"'.format(article_topics.count(topic),topic))\n",
    "                break\n",
    "            if title not in article_titles:\n",
    "                article_titles += [title]\n",
    "                article_contents += [content]\n",
    "                article_topics += [topic]\n",
    "                save_to_txt('title-{}'.format(n),title)\n",
    "                save_to_txt('article-{}'.format(n),content)\n",
    "                save_to_txt('topic-{}'.format(n),topic)\n",
    "                print('{}'.format(title))\n",
    "                n += 1\n",
    "                if round(len(article_titles)/10) == len(article_titles)/10:\n",
    "                    print('Article count: {}'.format(len(article_titles)))\n",
    "        except:\n",
    "            i += 1\n",
    "            if i == len(url_articles):\n",
    "                print('Only {} articles found in \\\"{}\"'.format(article_topics.count(topic),topic))\n",
    "                break\n",
    "            pass\n",
    "        \n",
    "                \n",
    "df = pd.DataFrame({'topic':article_topics,'title':article_titles,'content':article_contents})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the database stored in the directory we want, we can use the code provided in the case study to import the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T23:41:14.985453Z",
     "start_time": "2020-06-08T23:41:14.955254Z"
    }
   },
   "outputs": [],
   "source": [
    "#total number of articles to process\n",
    "N = 80\n",
    "#in memory stores for the topics, titles and contents of the news stories\n",
    "topics_array = []\n",
    "titles_array = []\n",
    "corpus = []\n",
    "for i in range(1, N+1):\n",
    "    #get the contents of the article\n",
    "    with open('Data/article-' + str(i) + '.txt', 'r') as myfile:\n",
    "        d1=myfile.read().replace('\\n', '')\n",
    "        d1 = d1.lower()\n",
    "        corpus.append(d1)\n",
    "    #get the original topic of the article\n",
    "    with open('Data/topic-' + str(i) + '.txt', 'r') as myfile:\n",
    "        to1=myfile.read().replace('\\n', '')\n",
    "        to1 = to1.lower()\n",
    "        topics_array.append(to1)\n",
    "    #get the title of the article\n",
    "    with open('Data/title-' + str(i) + '.txt', 'r') as myfile:\n",
    "        ti1=myfile.read().replace('\\n', '')\n",
    "        ti1 = ti1.lower()\n",
    "        titles_array.append(ti1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to do the following:\n",
    "1. Loop over all the article text corpuses to determine all the unique words used across our dataset.\n",
    "2. Find the subset of the entities from the NER model that are among the unique words being used across the dataset (determined in step 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T23:41:16.193431Z",
     "start_time": "2020-06-08T23:41:14.986752Z"
    }
   },
   "outputs": [],
   "source": [
    "#entity subset array\n",
    "entity_text_array = [] \n",
    "for i in range(1, N+1):\n",
    "    #load the article contents text file and convert it into a list of words.\n",
    "    tokens = tokenize(load_entire_file(('Data/article-' + str(i) + '.txt')))\n",
    "    #extract all entities known to the ner model mentioned in this article\n",
    "    entities = ner.extract_entities(tokens)\n",
    "    #extract the actual entity words and append to the array\n",
    "    for e in entities: \n",
    "        range_array = e[0]\n",
    "        tag = e[1]\n",
    "        score = e[2]\n",
    "        score_text = \"{:0.3f}\".format(score)\n",
    "        entity_text = \" \".join(tokens[j].decode(\"utf-8\") for j in range_array) \n",
    "        entity_text_array.append(entity_text.lower())\n",
    "#remove duplicate entities detected\n",
    "#entity_text_array = np.unique(entity_text_array)\n",
    "entity_text_array = list(set(entity_text_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-22T19:12:49.108073Z",
     "start_time": "2020-05-22T19:12:49.094235Z"
    }
   },
   "source": [
    "Now that we have the list of all entities used across our dataset, we can represent each article as a vector that contains the [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf ) score for each entity stored in the `entity_text_array`. This task can easily be achieved by using the [scikit-learn library](http://scikitlearn.org/stable/) for Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T23:41:16.239409Z",
     "start_time": "2020-06-08T23:41:16.194940Z"
    }
   },
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(sublinear_tf=True, max_df=0.5, analyzer='word',\n",
    "                       stop_words='english', vocabulary=entity_text_array)\n",
    "corpus_tf_idf = vect.fit_transform(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the articles represented as vectors of their TF-IDF scores, we are ready to perform Spectral Clustering on the articles. We can use the scikit-learn library for this purpose as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T23:41:16.294950Z",
     "start_time": "2020-06-08T23:41:16.242779Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpectralClustering(affinity='nearest_neighbors', assign_labels='kmeans',\n",
       "                   coef0=1, degree=3, eigen_solver='arpack', eigen_tol=0.0,\n",
       "                   gamma=1.0, kernel_params=None, n_clusters=8,\n",
       "                   n_components=None, n_init=10, n_jobs=None, n_neighbors=10,\n",
       "                   random_state=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change n_clusters to equal the number of clusters desired\n",
    "n_clusters = 8\n",
    "#spectral clustering\n",
    "spectral = cluster.SpectralClustering(n_clusters= n_clusters, \n",
    "                                      eigen_solver='arpack', \n",
    "                                      affinity=\"nearest_neighbors\", \n",
    "                                      n_neighbors = 10)\n",
    "spectral.fit(corpus_tf_idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the spectral clustering model fitted to the dataset. The following lines of code will help us see the output in the following format (one line per article):\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>__article_number, topic, spectral_clustering_cluster_number, article_title__</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T23:41:16.306088Z",
     "start_time": "2020-06-08T23:41:16.297600Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 elections_2020 6 uk diplomats fear end of special relationship if trump re-elected\n",
      "1 elections_2020 0 colin powell endorses joe biden for us president\n",
      "2 elections_2020 0 joe biden officially clinches democratic presidential nomination\n",
      "3 elections_2020 0 joe biden says '10-15%' of americans 'are just not very good people'\n",
      "4 elections_2020 0 trump tried to vote with wrong address while railing against voter fraud\n",
      "5 elections_2020 0 democrats unveil ambitious plan for police reform: 'this is a first step'\n",
      "6 elections_2020 0 trump and biden offer starkly different visions with nation at a crossroads\n",
      "7 elections_2020 0 america's seniors ebb away from trump as coronavirus response disappoints\n",
      "8 elections_2020 0 ‘it could have a chilling effect’: why trump is ramping up attacks on mail-in voting\n",
      "9 elections_2020 0 'you have to respond forcefully': can joe biden fight trump's brutal tactics?\n",
      "10 world 3 new york cautiously starts to reopen for business after coronavirus lockdown\n",
      "11 world 6 matt hancock hails coronavirus 'retreat' as uk deaths tumble\n",
      "12 world 3 workers in tokyo's red-light district to be tested for coronavirus after new spike\n",
      "13 world 0 prince andrew in war of words with us prosecutors over epstein\n",
      "14 world 6 trump move to take us troops out of germany 'a dangerous game'\n",
      "15 world 1 us has officially entered first recession since 2009\n",
      "16 world 1 labour's left uneasy with leader's view on tearing down colston statue\n",
      "17 world 3 sweden to present findings on olof palme assassination\n",
      "18 world 7 alarm at turkish plan to expand powers of nightwatchmen\n",
      "19 world 5 'it was hell': spanish cocaine raid adds to shipboard misery for 4,000 cows\n",
      "20 environment 3 $1m treasure in rocky mountains has been found, says forrest fenn\n",
      "21 environment 5 louisiana: coastal residents evacuated as tropical storm cristobal approaches\n",
      "22 environment 0 trump orders agencies cut environment reviews, citing 'economic emergency'\n",
      "23 environment 0 court overturns epa approval of popular herbicide made by monsanto\n",
      "24 environment 5 us ranks 24th in the world on environmental performance\n",
      "25 environment 0 renewables surpass coal in us energy generation for first time in 130 years\n",
      "26 environment 6 firms ignoring climate crisis will go bankrupt, says mark carney\n",
      "27 environment 1 walking app helps tree lovers know their sycamores from their maples\n",
      "28 environment 5 many of the 300 plants and animals endemic to canada at risk, report finds\n",
      "29 environment 3 why can't we leave them alone? the troubling truth about selfies with sloths\n",
      "30 soccer 2 raheem sterling demands english football gives black managers a chance\n",
      "31 soccer 2 premier league restart preview no 3: bournemouth\n",
      "32 soccer 4 billy kee: 'if i didn’t opt out of football, i don’t know if i would be alive'\n",
      "33 soccer 2 merseyside derby in limbo after safety group postpones goodison decision\n",
      "34 soccer 2 bayern remind leverkusen of just where they sit in the pecking order\n",
      "35 soccer 2 premier league restart preview no 2: aston villa\n",
      "36 soccer 2 premier league restart preview no 1: arsenal\n",
      "37 soccer 5 pep clotet to step down as birmingham manager at end of season\n",
      "38 soccer 2 premier league clubs turn to online dating methods in the transfer market\n",
      "39 soccer 5 my favourite game: when denmark beat uruguay 6-1 at the 86 world cup\n",
      "40 us_politics 0 trump plans to resume election rallies despite warnings over large crowds\n",
      "41 us_politics 0 ‘apathy is no longer a choice’: will the george floyd protests energize young voters?\n",
      "42 us_politics 1 ‘they set us up’: us police arrested over 10,000 protesters, many non-violent\n",
      "43 us_politics 1 minneapolis lawmakers vow to disband police department in historic move\n",
      "44 us_politics 0 racism in america is not the exception – it's the norm\n",
      "45 us_politics 0 america's top cop is a rightwing culture warrior who hates disorder. what could go wrong?\n",
      "46 us_politics 0 a week that shook a nation: anger burns as power of protests leaves trump exposed\n",
      "47 us_politics 5 iowa touted its covid-19 testing. now officials are calling for an investigation\n",
      "48 us_politics 0 'as guarded as fort knox': the inside story of hillary clinton's presidential campaign\n",
      "49 us_politics 0 'how did we get here?': trump has normalised mayhem and the us is paying the price\n",
      "50 business 6 bp to cut 10,000 jobs worldwide amid huge drop in demand for oil\n",
      "51 business 3 world bank warns covid-19 pandemic risks dramatic rise in poverty\n",
      "52 business 7 covid-19 vaccine: astrazeneca has 'approached gilead over possible merger'\n",
      "53 business 3 behind the us unemployment figures: five key points from may's jobs report\n",
      "54 business 3 the cares act's best-kept secret is a boon not just for survival but for growth\n",
      "55 business 6 angela merkel has become the spend, spend, spend chancellor\n",
      "56 business 1 the pandemic exposes us childcare for what it is: ‘a crisis within a crisis’\n",
      "57 business 3 coronavirus solves one problem for us employers: finding workers\n",
      "58 business 6 europe’s big two kiss and make up for pandemic rescue deal\n",
      "59 business 1 england could suspend sunday trading laws in push to boost economy\n",
      "60 tech 4 facebook moderators join criticism of zuckerberg over trump stance\n",
      "61 tech 6 smart appliances may not be worth money in long run, warns which?\n",
      "62 tech 7 samsung galaxy z flip review: four months with the folding phone\n",
      "63 tech 7 zoom to exclude free calls from end-to-end encryption to allow fbi cooperation\n",
      "64 tech 4 embarrassing teenage posts on facebook? now you can delete them\n",
      "65 tech 4 more than 140 zuckerberg-funded scientists call on facebook to rein in trump\n",
      "66 tech 4 zuckerberg: facebook will review policies after backlash over trump posts\n",
      "67 tech 7 zoom booms as teleconferencing company profits from coronavirus crisis\n",
      "68 tech 6 hackers targeting uk research labs amid vaccine race – gchq chief\n",
      "69 tech 0 new vulnerability allows users to 'jailbreak' iphones\n",
      "70 science 3 gardens of the galaxy: can you grow vegetables on mars?\n",
      "71 science 1 we can no longer ignore the potential of psychedelic drugs to treat depression\n",
      "72 science 1 starwatch: how to find the great diamond in the sky\n",
      "73 science 1 we often accuse the right of distorting science. but the left changed the coronavirus narrative overnight\n",
      "74 science 1 after six months of coronavirus, how close are we to defeating it?\n",
      "75 science 6 retracted studies may have damaged public trust in science, top researchers fear\n",
      "76 science 5 the first wave of covid-19 is not over – but how might a second look?\n",
      "77 science 3 hydroxychloroquine and coronavirus: a guide to the scientific studies so far\n",
      "78 science 6 uk minister hails 'game-changing' coronavirus immunity test\n",
      "79 science 6 uk ministers face legal challenge for refusal to order ppe inquiry\n"
     ]
    }
   ],
   "source": [
    "if hasattr(spectral, 'labels_'):\n",
    "    cluster_assignments = spectral.labels_.astype(np.int)\n",
    "    for i in range(0, len(cluster_assignments)):\n",
    "        print (i, topics_array[i], cluster_assignments [i], titles_array[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T23:41:16.330138Z",
     "start_time": "2020-06-08T23:41:16.308197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictions_0</th>\n",
       "      <th>predictions_1</th>\n",
       "      <th>predictions_2</th>\n",
       "      <th>predictions_3</th>\n",
       "      <th>predictions_4</th>\n",
       "      <th>predictions_5</th>\n",
       "      <th>predictions_6</th>\n",
       "      <th>predictions_7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Business</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Elections_2020</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Environment</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Science</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Soccer</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Tech</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>US_Politics</th>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>World</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                predictions_0  predictions_1  predictions_2  predictions_3  \\\n",
       "topic                                                                        \n",
       "Business                    0              2              0              4   \n",
       "Elections_2020              9              0              0              0   \n",
       "Environment                 3              1              0              2   \n",
       "Science                     0              4              0              2   \n",
       "Soccer                      0              0              7              0   \n",
       "Tech                        1              0              0              0   \n",
       "US_Politics                 7              2              0              0   \n",
       "World                       1              2              0              3   \n",
       "\n",
       "                predictions_4  predictions_5  predictions_6  predictions_7  \n",
       "topic                                                                       \n",
       "Business                    0              0              3              1  \n",
       "Elections_2020              0              0              1              0  \n",
       "Environment                 0              3              1              0  \n",
       "Science                     0              1              3              0  \n",
       "Soccer                      1              2              0              0  \n",
       "Tech                        4              0              2              3  \n",
       "US_Politics                 0              1              0              0  \n",
       "World                       0              1              2              1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['predictions'] = cluster_assignments\n",
    "predictions_df = pd.get_dummies(df, columns=['predictions']).drop(['title','content'],axis=1).groupby(['topic']).sum()\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be observed, the algorithm does not classify the articles according to the newspaper sections they have been taken from. You can take a further look at the model parameters in order to improve those results, or find insights about the criteria the algorithm is currently using to cluster the articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "Created by Iñigo de la Maza. Contact: [idelamaza.com](http://www.idelamaza.com)\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-08T23:41:16.730167Z",
     "start_time": "2020-06-08T23:41:16.332319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of packages and versions:\n",
      "\n",
      "scikit-learn==0.22.1\n",
      "requests==2.23.0\n",
      "pandas==1.0.0\n",
      "numpy==1.18.1\n",
      "mitie==0.7.36\n",
      "matplotlib==3.1.3\n",
      "beautifulsoup4==4.9.1\n"
     ]
    }
   ],
   "source": [
    "## IGNORE THE CODE BELOW ##\n",
    "\n",
    "#Getting names of imported libraries and versions for creating a requirements.txt file\n",
    "import pkg_resources\n",
    "import types\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "            \n",
    "        poorly_named_packages = {\n",
    "            \"PIL\": \"Pillow\",\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "requirements.append(('beautifulsoup4', '4.9.1'))    \n",
    "#Getting the packages already included in requirements.txt\n",
    "with open(r\"../../requirements.txt\", \"r\") as f:\n",
    "    pkgs = [pkg.split('==')[0] for pkg in f.readlines()]\n",
    "#Adding missing packages\n",
    "print('List of packages and versions:\\n')     \n",
    "with open(r\"../../requirements.txt\", \"a\") as f:\n",
    "    for r in requirements:\n",
    "        print(\"{}=={}\".format(*r))\n",
    "        if r[0] not in pkgs:\n",
    "            f.write(\"{}=={}\\n\".format(*r))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
